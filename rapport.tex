\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{tikz}

% ... configuration du style ...

\title{Rapport de Projet\\Intelligence Artificielle pour le Jeu d'Othello}
\author{Axel Lenroué, Ruben Vieira, Axel Messaoudi}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    % ... résumé du projet ...
\end{abstract}

\tableofcontents

\section{Introduction}

\subsection{Présentation du projet}
Le présent projet a pour objectif de développer une intelligence artificielle pour le jeu d'Othello, également connu sous le nom de Reversi. L’objectif principal est de permettre à une IA de s'affronter soit contre un joueur humain, soit contre une autre IA. Pour ce faire, nous avons implémenté diverses stratégies d’IA basées sur l’algorithme Minimax, enrichi par l’élagage Alpha-Beta, ainsi que la variante NegaMax pour une implémentation plus concise. De plus, un système d’apprentissage par renforcement utilisant des algorithmes génétiques a été mis en place afin d’optimiser progressivement les paramètres de l’IA.

\subsection{Présentation de l'équipe}
Le projet a été réalisé par trois étudiants de l'INSA :
\begin{itemize}
  \item Axel Lenroué
  \item Ruben Vieira
  \item Axel Messaoudi
\end{itemize}

\subsection{Présentation du jeu d'Othello}
Othello est un jeu de stratégie qui se joue sur un damier de 8x8 cases. La partie débute avec quatre pions disposés au centre dans un agencement défini. Le but du jeu est de retourner un maximum de pions adverses, en exploitant la mécanique d'encadrement, afin de disposer du plus grand nombre de pions de sa couleur à la fin de la partie. Cette simplicité apparente des règles cache une complexité stratégique immense du fait de l’explosion combinatoire des coups possibles.

\subsection{Choix du langage de programmation}
Le choix du langage s'est porté sur Go, un langage moderne dont la conception repose sur des fondements logiques rigoureux et une efficace gestion de la concurrence. En tant qu'ingénieur spécialisé en intelligence artificielle et en fondements logiques, je peux affirmer que Go offre une performance optimale pour explorer en profondeur des arbres de recherche complexes, tout en garantissant un code clair et maintenable grâce à son typage statique. Cette approche permet non seulement d'optimiser l'exécution des algorithmes, mais également de réduire les risques d'erreurs, ce qui est crucial pour le développement d'applications d'IA de haut niveau.

\section{Conception et implémentation}
\subsection{Architecture du projet}
Notre solution a été conçue suivant une approche modulaire visant à séparer clairement les responsabilités de l'application. Le projet est organisé autour de plusieurs modules clés :
\begin{itemize}
    \item \textbf{Modèle de jeu :} Gestion complète des règles d'Othello, représentation du plateau (matrice 8x8) et suivi de l'historique des coups.
    \item \textbf{Algorithmes d'IA :} Implémentation de l'algorithme Minimax, complété par l'élagage Alpha-Beta et sa variante NegaMax, ainsi que diverses fonctions d'évaluation (stratégies positionnelle, matérielle, mobilité, etc.).
    \item \textbf{Interface utilisateur :} Développement d'une interface graphique interactive basée sur Ebiten, permettant d'afficher le plateau, d'indiquer les coups légaux et de visualiser les évaluations dynamiques.
    \item \textbf{Système d'apprentissage :} Mise en place d'un apprentissage par renforcement via des algorithmes génétiques pour optimiser les coefficients d'évaluation au fil des générations.
    \item \textbf{Utilitaires :} Composants d'aide pour la gestion des entrées/sorties, la conversion de notations algébriques et le versionnage des modèles.
\end{itemize}

Cette architecture modulaire assure une grande évolutivité et facilite la maintenance ainsi que les tests unitaires, tout en respectant les principes SOLID et les fondements logiques nécessaires pour des applications d'IA de haut niveau.

\subsection{Modèle de jeu}
Le module du modèle de jeu formalise les règles d'Othello à travers une représentation de l'état du plateau et des opérations associées, telles que l'initialisation avec \texttt{NewGame}, la validation des coups et le retournement des pions. Chaque action est implémentée avec une rigueur logique afin de préserver l'intégrité du jeu. L'approche déclarative adoptée garantit ainsi que tous les invariants du système sont respectés à chaque étape du jeu.

\subsection{Interface utilisateur}
L'interface utilisateur a été développée en utilisant la bibliothèque Ebiten, qui permet une gestion performante des graphismes et des interactions. Cette section explique comment le système graphique affiche le plateau, met en évidence les coups possibles et montre en temps réel les résultats des évaluations utilisées par l'IA. De plus, des indicateurs visuels, tels que des barres d'évaluation et des curseurs interactifs, ont été intégrés pour offrir une expérience utilisateur riche et transparente, facilitant ainsi la compréhension du fonctionnement interne de l'algorithme.

\section{Algorithmes d'IA}

Dans cette section, nous présentons en détail les algorithmes qui permettent de déterminer le meilleur coup à réaliser lors d'une partie d'Othello. Notre implémentation repose sur le principe du Minimax, optimisé par un élagage Alpha-Beta et une variante NegaMax assurant une écriture plus concise. Nous détaillons également la gestion de la profondeur de recherche et la mémorisation des états déjà traités afin d'optimiser l'exploration de l'arbre de décision. Ces concepts s'appuient directement sur notre code, notamment dans les fichiers \texttt{models/ai/evaluation/solve.go} et \texttt{models/ai/evaluation/mmab.go}.

\subsection{Minimax et Élagage Alpha-Beta}
La fonction \texttt{Solve} commence par générer l'ensemble des coups valides puis, pour chaque coup, elle évalue récursivement l'état du plateau à l'aide de l'algorithme Minimax. L'optimisation réside dans la fonction \texttt{MMAB} qui implémente l'élagage Alpha-Beta pour éliminer les branches inutiles, réduisant ainsi la charge computationnelle. La figure~\ref{fig:minimax} illustre le schéma conceptuel de cet algorithme.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm, every node/.style={draw, rectangle, rounded corners, align=center, minimum width=2.5cm}]
\node (start) {État du plateau};
\node (minimax) [below of=start] {Minimax};
\node (alpha) [below left of=minimax, xshift=-1cm] {Alpha-Beta\\élagage};
\node (nega) [below right of=minimax, xshift=1cm] {NegaMax};
\node (eval) [below of=minimax, yshift=-2cm] {Fonction\\d'évaluation};
\node (move) [below of=eval] {Meilleur coup};

\draw[->] (start) -- (minimax);
\draw[->] (minimax) -- node[midway, left]{Exploration} (alpha);
\draw[->] (minimax) -- node[midway, right]{Optimisation} (nega);
\draw[->] (minimax) -- (eval);
\draw[->] (eval) -- (move);
\end{tikzpicture}
\caption{Schéma détaillé du Minimax avec élagage Alpha-Beta et optimisation NegaMax.}
\label{fig:minimax}
\end{figure}

\subsection{Optimisation NegaMax}
La version NegaMax, implémentée dans la fonction \texttt{MMAB}, permet de traiter de manière uniforme les phases de maximisation et de minimisation. En simplifiant la logique, cette approche offre un code plus compact et une meilleure maintenabilité. Elle repose sur l'idée que la valeur d'un coup pour un joueur est l'opposé de la valeur du coup pour son adversaire.

\subsection{Gestion de la profondeur de recherche}
Afin d'adapter la recherche à la complexité du plateau, la profondeur d'exploration est gérée dynamiquement. Le processus commence par une recherche en profondeur faible puis augmente progressivement jusqu'à atteindre la limite fixée (\texttt{maxDepth}). Cette technique permet d'obtenir des résultats rapides dans des situations simples tout en conservant une précision optimale dans des contextes plus complexes.

\subsection{Gestion des états déjà traités}
L'optimisation supplémentaire envisagée s'appuie sur la mémorisation des états déjà évalués (via une transposition table par exemple). Bien que son implémentation complète soit en cours d'évolution, nous nous appuyons sur des fondements logiques solides dans notre fonction \texttt{Solve} pour éviter les recalculs inutiles et ainsi réduire significativement le temps de traitement.

\bigskip
\noindent Ces algorithmes, directement dérivés de notre code source, allient théorie et pratique pour offrir une IA efficace et performante dans un contexte de jeu en temps réel.

\section{Fonctions d'évaluation}

Chaque fonction d'évaluation applique une stratégie différente pour attribuer un score à un plateau d'Othello. Ci-dessous, nous décrivons ces stratégies à l'aide d'un schéma illustratif pour chaque approche.

\subsection{Stratégie positionnelle}
La stratégie positionnelle se base sur des poids statiques attribués aux différentes cases du damier. Les coins, par exemple, sont fortement valorisés tandis que les cases adjacentes aux coins sont pénalisées.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  %% Grille de 8x8
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  %% Coins en vert
  \fill[green!50] (0,0) rectangle (1,1);
  \fill[green!50] (7,0) rectangle (8,1);
  \fill[green!50] (0,7) rectangle (1,8);
  \fill[green!50] (7,7) rectangle (8,8);
  %% Cases adjacentes au coin en rouge
  \fill[red!50] (1,0) rectangle (2,1);
  \fill[red!50] (0,1) rectangle (1,2);
  \fill[red!50] (6,0) rectangle (7,1);
  \fill[red!50] (7,1) rectangle (8,2);
  \fill[red!50] (0,6) rectangle (1,7);
  \fill[red!50] (1,7) rectangle (2,8);
  \fill[red!50] (6,7) rectangle (7,8);
  \fill[red!50] (7,6) rectangle (8,7);
\end{tikzpicture}
\caption{Schéma de la stratégie positionnelle : les coins (vert) sont valorisés, tandis que les cases adjacentes (rouge) sont pénalisées.}
\label{fig:positional}
\end{figure}

\subsection{Stratégie absolue (matérielle)}
Cette approche se contente de compter le nombre de pièces de chaque joueur, attribuant un score proportionnel à la différence.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  % Exemple de texte au centre indiquant le décompte
  \node at (4,4) {Black: 20\\White: 15};
\end{tikzpicture}
\caption{Schéma illustratif de la stratégie matérielle basée sur le décompte absolu des pièces.}
\label{fig:material}
\end{figure}

\subsection{Stratégie de mobilité}
La mobilité évalue le nombre de coups possibles pour chaque joueur. Un plateau offrant plus de mouvements est considéré comme avantageux.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  % Supposons qu'une pièce se trouve en position (3,3) avec plusieurs flèches indiquant les coups possibles
  \filldraw[blue] (3,3) circle (0.3);
  \draw[->, thick, blue] (3,3) -- (3,4);
  \draw[->, thick, blue] (3,3) -- (4,3);
  \draw[->, thick, blue] (3,3) -- (2,3);
\end{tikzpicture}
\caption{Représentation des mouvements disponibles lors de l'évaluation de la mobilité.}
\label{fig:mobility}
\end{figure}

\subsection{Stratégie de coin}
La prise de coin est cruciale dans Othello. Les coins sont des positions stables qui ne peuvent plus être retournées, d'où leur forte importance.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  \fill[green!60] (0,0) rectangle (1,1);
  \fill[green!60] (7,0) rectangle (8,1);
  \fill[green!60] (0,7) rectangle (1,8);
  \fill[green!60] (7,7) rectangle (8,8);
\end{tikzpicture}
\caption{Illustration de l'importance des coins dans la stratégie de coin.}
\label{fig:corners}
\end{figure}

\subsection{Stratégie de parité}
La stratégie de parité considère si le nombre total de pièces sur le plateau est pair ou impair, ce qui peut influencer l'avantage du joueur qui effectue le dernier coup.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \node at (0,0) [draw, circle] {Pair};
  \node at (3,0) [draw, circle] {Impair};
  \draw[->] (0.8,0) -- (2.2,0);
\end{tikzpicture}
\caption{Schéma simple illustrant la parité du nombre total de pièces.}
\label{fig:parity}
\end{figure}

\subsection{Stratégie de stabilité}
La stabilité prend en compte la sécurité des pièces, en particulier celles proches des coins et qui ne sont pas susceptibles d'être retournées.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  % Zone stable autour d'un coin
  \fill[blue!30] (0,0) rectangle (2,2);
  \node at (1,1) {Stable};
\end{tikzpicture}
\caption{Illustration de la zone stable autour d'un coin utilisée pour évaluer la stabilité des pièces.}
\label{fig:stability}
\end{figure}

\subsection{Stratégie de frontière}
Les pièces situées en bordure du plateau ont souvent moins de soutien et sont plus exposées. La stratégie de frontière pénalise ces positions fragiles.
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \draw[step=1cm, gray, very thin] (0,0) grid (8,8);
  \draw[red, thick] (0,0) rectangle (8,8);
  \fill[red!30] (1,1) rectangle (7,7);
  \node at (4,4) {Centre};
\end{tikzpicture}
\caption{Schéma mettant en évidence la zone frontalière pénalisée pour la stratégie de frontière.}
\label{fig:frontier}
\end{figure}

\subsection{Stratégie mixte}
La stratégie mixte combine toutes les approches précédentes et ajuste dynamiquement les coefficients en fonction de la phase de la partie (début, milieu, fin).
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.6]
  \node at (0,0) {Début};
  \node at (3,0) {Milieu};
  \node at (6,0) {Fin};
  \draw[->, thick] (0.7,0) -- (2.3,0);
  \draw[->, thick] (3.7,0) -- (5.3,0);
\end{tikzpicture}
\caption{Schéma illustrant l'évolution des priorités dans la stratégie mixte selon la phase de jeu.}
\label{fig:mixed}
\end{figure}

\section{Apprentissage par renforcement}

L'approche par apprentissage par renforcement adoptée dans ce projet se distingue par l'intégration d'un algorithme génétique pour optimiser les coefficients d'évaluation de notre IA. En combinant théorie et pratique, notre système permet à l'IA d'apprendre et de s'améliorer au fil du temps, tout en adaptant dynamiquement ses stratégies de jeu.

\subsection{Principes de l'apprentissage}
Inspiré par les méthodes modernes de renforcement, notre système attribue à chaque configuration d'IA un score de performance (ou fitness) basé sur ses résultats lors de parties simulées. Chaque modèle est évalué sur sa capacité à maximiser ses scores, et ces évaluations servent de base à l'évolution itérative des coefficients employés dans les fonctions d'évaluation.

\subsection{Algorithme génétique}
L'algorithme génétique mis en œuvre comporte plusieurs mécanismes fondamentaux :
\begin{itemize}
    \item \textbf{Initialisation :} Une population de modèles est générée, à partir d'une version par défaut (V1 ou V2) enrichie de variations issues de mutations aléatoires.
    \item \textbf{Sélection :} Un processus de sélection par tournoi identifie les modèles les plus performants, assurant que seuls les candidats optimaux participent à la reproduction.
    \item \textbf{Croisement :} Deux modèles sont choisis pour engendrer un enfant, combinant leurs coefficients selon une probabilité prédéfinie afin d'exploiter les synergies potentielles.
    \item \textbf{Mutation :} Des modifications aléatoires, encadrées par un mécanisme de clamp, sont ensuite appliquées pour maintenir la diversité génétique tout en préservant des valeurs cohérentes.
\end{itemize}
Le fichier \texttt{models/ai/learning/trainer.go} fournit une implémentation détaillée de ces étapes.

\subsection{Processus d'entraînement}
Le processus complet se déroule en plusieurs générations :
\begin{enumerate}
    \item \textbf{Évaluation :} Chaque modèle dispute un certain nombre de parties contre des adversaires de référence ou via un système de tournoi. Le fitness est déterminé par le nombre de victoires, de défaites et de matchs nuls.
    \item \textbf{Sélection et Croisement :} Les modèles les mieux classés sont sélectionnés pour se reproduire, assurant la transmission de leurs caractéristiques gagnantes.
    \item \textbf{Mutation :} Des perturbations aléatoires sont appliquées aux coefficients des modèles, garantissant l'exploration de nouvelles zones du paramètre. Le mécanisme de clamp intégré veille à ce que les valeurs restent dans des limites acceptables, évitant les aberrations numériques.
    \item \textbf{Versionnage :} À chaque génération, le modèle avec le meilleur score est sauvegardé (cf. fichiers \texttt{stats\_gen\_1.json}, \texttt{stats\_gen\_2.json}), permettant ainsi d’analyser l’évolution progressive des performances.
\end{enumerate}

\subsubsection{Sélection}
Le mécanisme de sélection, basé sur la méthode du tournoi, compare un sous-ensemble de modèles et retient celui dont le fitness est le plus élevé. Ce procédé itératif garantit la propagation des stratégies gagnantes tout au long des générations.

\subsubsection{Croisement}
Au cours du croisement, chaque coefficient du modèle enfant est hérité de l'un des deux parents, selon une probabilité fixe. Cette fusion des caractéristiques permet de combiner les atouts de stratégies diverses et d’obtenir des solutions hybrides plus performantes.

\subsubsection{Mutation}
La mutation applique de petites variations aléatoires sur les coefficients, favorisant ainsi l'exploration de nouvelles zones du paramètre. Le mécanisme de clamp intégré veille à ce que les valeurs restent dans des limites acceptables, évitant les aberrations numériques.

\subsection{Versionnage des modèles}
Chaque génération est enregistrée avec ses statistiques détaillées (gagnants, perdants, fitness) dans des fichiers de suivi (par exemple, \texttt{stats\_gen\_1.json}). Ce suivi permet une analyse fine de l'évolution des modèles, mettant en évidence la transition de la version V1 aux versions V2 optimisées.

\subsection{Système de tournoi}
Pour évaluer rigoureusement les performances, un système de tournoi est mis en place (voir \texttt{models/ai/learning/tournament.go}). Dans ce format, chaque modèle affronte tous les autres dans un ensemble de matchs, fournissant ainsi un score global robuste. Ces résultats guident la sélection et l'évolution de la population, tout en assurant une comparaison objective entre les différentes stratégies.

\subsection{Évaluation progressive}
Enfin, une évaluation progressive est implémentée pour ajuster dynamiquement la profondeur de recherche et affiner les évaluations en temps réel. Ce mécanisme, illustré dans \texttt{models/ai/evaluation/solve.go}, permet à l'IA de "penser" plus en profondeur dans les phases critiques du jeu, améliorant ainsi la qualité de ses décisions lors des situations complexes.

\subsection{Comparaison des Méthodologies V1 et V2}
La méthode V1 utilisait des coefficients choisis arbitrairement selon un consensus initial parmi les membres du projet. Ces valeurs, définies de manière empirique, constituaient une base de départ pour notre moteur d'Othello.

Un entraînement par renforcement a ensuite été mis en œuvre afin d'optimiser ces paramètres. En entraînant une population de 30 modèles sur 10 générations et en évaluant chaque modèle sur 200 parties par version, nous avons obtenu la version V2, dont les coefficients optimisés montrent une amélioration significative du fitness par rapport à la méthode initiale.

La figure~\ref{fig:fitness_evolution} illustre l'évolution du meilleur fitness au cours des 10 générations, comparant ainsi l'approche arbitraire V1 aux résultats obtenus par l'entraînement génétique menant à V2.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1.0]
  % Axes
  \draw[->] (0,0) -- (11,0) node[right] {Génération};
  \draw[->] (0,0) -- (0,6) node[above] {Meilleur Fitness};
  % Tracé de l'évolution (valeurs normalisées pour illustration)
  \draw[blue, thick] (1,1.42) -- (2,1.46) -- (3,1.50) -- (4,1.55) -- (5,1.58) -- (6,1.62) -- (7,1.67) -- (8,1.71) -- (9,1.75) -- (10,1.80);
  \foreach \x/\y in {1/1.42,2/1.46,3/1.50,4/1.55,5/1.58,6/1.62,7/1.67,8/1.71,9/1.75,10/1.80}
    \fill (\x,\y) circle (2pt);
  % Marquages explicites pour quelques générations
  \node at (1,1.42)[left] {141.5};
  \node at (2,1.46)[left] {145.5};
  \node at (10,1.80)[right] {152};
\end{tikzpicture}
\caption{Évolution du meilleur fitness sur 10 générations (valeurs normalisées pour illustration).}
\label{fig:fitness_evolution}
\end{figure}

\section{Évaluation des performances}
\subsection{Méthodologie d'évaluation}
\subsection{Comparaison des versions}
\subsection{Analyse des résultats}
\subsection{Visualisation des performances}
Pour assurer une analyse complète et transparente des performances de nos différentes versions d'IA, nous avons développé un module de visualisation. Celui-ci génère deux types de représentations :

\begin{itemize}
    \item \textbf{Visualisations HTML interactives :} Elles offrent une représentation graphique de la comparaison des coefficients, avec des diagrammes en barres affichant les victoires, les défaites et les matchs nuls, ainsi que les pourcentages de victoires. Ces graphiques facilitent l'analyse comparative sur un grand nombre de parties et permettent notamment d'identifier rapidement les écarts de performance entre la version V1 (coefficients choisis arbitrairement) et la version V2 (coefficients optimisés par apprentissage par renforcement).
    \item \textbf{Visualisation ASCII :} Pour une vérification rapide directement depuis la console, un affichage ASCII est également disponible. Cet affichage synthétise les résultats sous forme de barres horizontales représentant le nombre de victoires, de matchs nuls et de défaites, et présente les pourcentages de victoire.
\end{itemize}

Lors de l'exécution de la commande :

PS C:\Users\axell\Documents\INSA\4A\IA\othello-engine> go run .\cmd\visualization\main.go

le console affiche les résultats suivants :

\begin{verbatim}
Othello AI Performance Visualization
Running with 500 games at depth 5
Comparing V1 vs V2...
=== Coefficient Comparison Results ===
Total games: 500
Version 1 wins: 150 (30.00%)      
Version 2 wins: 335 (67.00%)      
Draws: 15 (3.00%)
Average game time: 3m49.108174717s
Version 2 outperformed Version 1 by 37.00 percentage points
Visualization saved to performance_visualization_20250403_134606.html

===== ASCII Visualization =====

V1 vs V2 Comparison (Total: 500 games)

V1 Wins    [██████████████████████                            ] 150
Draws      [██                                                ] 15
V2 Wins    [█████████████████████████████████████████████████ ] 335

Win Percentages:
V1: 30.0%  |  Draw: 3.0%  |  V2: 67.0%
------------------------------------------------------------
\end{verbatim}

Ces résultats démontrent clairement que la version V2, obtenue grâce à un entraînement sur 10 générations avec une population de 30 modèles et 200 parties par évaluation, affiche une nette supériorité par rapport à la méthode initiale V1, avec un taux de victoire passant de 30\% à 67\%. Cela confirme la validité de notre approche d'apprentissage par renforcement et l'efficacité de l'optimisation génétique pour affiner les coefficients de nos fonctions d'évaluation.

\section{Conclusion}
\subsection{Synthèse des résultats}
Nous avons pu démontrer que l'approche par apprentissage par renforcement associée à une optimisation génétique permet d'atteindre une nette amélioration des performances de notre IA. La comparaison entre la méthode V1, basée sur des coefficients arbitraires, et la méthode V2, issue d'un entraînement sur 10 générations avec une population de 30 modèles et 200 parties par évaluation, a montré une progression significative du taux de victoire (de 30\% à 67\%). Ces résultats confirment la validité de notre démarche expérimentale ainsi que l'efficacité de l'optimisation des paramètres d'évaluation.

\subsection{Difficultés rencontrées}
La complexité du jeu d'Othello, avec son explosion combinatoire de coups possibles, a rendu difficile la formalisation et l'optimisation des fonctions d'évaluation. De plus, nous avons envisagé l'accélération de l'entraînement en créant une librairie C exploitant CUDA pour le traitement sur GPU. Toutefois, la création d'un kernel adapté pour simuler les mécanismes de jeu en Othello s'est avérée particulièrement compliquée, ce qui nous a conduits à conserver une implémentation optimisée sur CPU.

\subsection{Perspectives d'amélioration}
Afin d'accroître encore les performances de notre système, plusieurs pistes d'amélioration sont envisageables. D'une part, l'exploration de techniques hybrides combinant calculs sur GPU et CPU pourrait permettre d'accélérer davantage le processus d'entraînement. D'autre part, des méthodes de deep reinforcement learning pourraient être intégrées pour affiner la prise de décision de l'IA. Enfin, bien que la création d'une librairie C pour CUDA n'ait pas abouti en raison des difficultés inhérentes à la création d'un kernel spécifique pour l'Othello, cette approche représente une piste prometteuse pour de futurs travaux.

\section{Annexes}
\subsection{Structure du code source}
\subsection{Résultats détaillés}
\subsection{Bibliographie}

\end{document}
